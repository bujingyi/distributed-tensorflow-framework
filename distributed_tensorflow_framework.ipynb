{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Distributed TensorFlow Framework\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall there are two main strategies for distributing deep learning:\n",
    "* **Model Parallelism**\n",
    "* **Data Parallelism**\n",
    "\n",
    "**Model Parallelism** is to partition the model across several machines by assigning different nodes to different machines, if the model is too big to fit into the memory on one machine. The benefits is obvious that model parallelism allows us to train extremely large models while the downside of this approach is that one partitioned part has to wait for others during updating.\n",
    "<div align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/bujingyi/distributed-tensorflow-framework/master/model_parallelism.png\" >\n",
    " </div>\n",
    "\n",
    "**Data Parallelism** is to distribute training multiple model instances which are assigned on different machines. This approach leads to the parallel architecture called \"Parameter Sever (ps) - Worker\". Parameter servers will store the entire graph and collect update operations from multiple workers where data batches will be read and gradients will be computed. \n",
    "<div align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/bujingyi/distributed-tensorflow-framework/master/data_parallelism.png\" >\n",
    " </div>\n",
    "\n",
    "Data Parallelism has two ways of training:\n",
    "\n",
    "* **Synchronous**: all workers read the same parameters from ps and compute updates synchronously.\n",
    "* **Asynchronous**: workers read from ps (probably different values for the graph parameters), compute and send updates asynchronously.\n",
    "\n",
    "TensorFlow provides a easy way to implement **Asynchronous Data Parallelism**. This notebook shows how to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asunchronous Data Parallelism with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only three steps to distribute TensorFlow:\n",
    "1. Define your compute cluster via `tf.trainClusterSpec` and `tf.train.Server`\n",
    "2. Assgin your model to ps and workers via `tf.device` and `tf.train.replica_device_setter`\n",
    "3. Launch a distributed TensorFlow session via `tf.train.MonitoredTrainingSession`\n",
    "\n",
    "Let's go through them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define your compute cluster via tf.trainClusterSpec and tf.train.Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to share variables between machines, you need to let each member in your cluster know the existence and organization of the cluster. \n",
    "\n",
    "With Distributed TensorFlow, each process runs a special execution engine, a TensorFlow server. Servers are linked together as part of a cluster. Each server in the cluster is also known as a task. For example, if we have three computers with IP address 10.189.253.1, 10.189.253.3, 10.189.253.2 repestively, we could define a cluster composed of one ps and two workers as below\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_hosts = ['10.189.253.1:2222']\n",
    "worker_hosts = ['10.189.253.2:2222', '10.189.253.3:2222']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each task is associated with a role, which is a collection of related tasks. We associate the three tasks with one job called 'ps', and two jobs called 'worker'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roles = {\n",
    "    'ps': ps_hosts,\n",
    "    'worker': worker_hosts\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can define the cluster by assigning jobs to tasks (servers) via `tf.train.ClusterSpec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a cluster specification to describe the cluster\n",
    "cluster = tf.train.ClusterSpec(roles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have specified our cluster, next is to tell TensorFlow to create servers based on the cluster specification (here we us ps as an example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = tf.train.Server(\n",
    "        server_or_cluster_def=cluster,\n",
    "        job_name='ps',\n",
    "        task_index=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quoted from TensorFlow's offical doc:\n",
    ">A tf.train.ClusterSpec represents the set of processes that participate in a distributed TensorFlow computation. Every tf.train.Server is constructed in a particular cluster.\n",
    "\n",
    ">A `tf.train.Server` instance encapsulates a set of devices and a `tf.Session` target that can participate in distributed training. A server belongs to a cluster (specified by a `tf.train.ClusterSpec`), and corresponds to a particular task in a named job. The server can communicate with any other server in the same cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Assgin your model to ps and workers via tf.device and tf.train.replica_device_setter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining cluster and creating servers, you should now assign graph nodes including both operations and variables to a specific server using `tf.device`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\n",
    "        tf.train.replica_device_setter(\n",
    "            worker_device='job:worker/task:{}'.format(0),  # 0 is the task_index\n",
    "            cluster=cluster\n",
    "        )\n",
    "):\n",
    "    # create the computation graph\n",
    "    input_placeholder = ...  # placeholders\n",
    "    layer = ...  # neural network layers\n",
    "    prediction = ...  # predictions\n",
    "    loss = ...  # loss\n",
    "    train_op = ...  # train operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Data Parallelism, variables should be assigned to `ps` and operations to `workers`. Fortunately we don't have to manually manipulate them. TensorFlow automatically take care of it by `tf.train.replica_device_setter`.\n",
    ">`tf.train.replica_device_setter` returns a device function to use when building a Graph for replicas. Device Functions are used in with tf.device(device_function): statement to automatically assign devices to Operation objects as they are constructed, Device constraints are added from the inner-most context first, working outwards. The merging behavior adds constraints to fields that are yet unset by a more inner context. Currently the fields are (job, task, cpu/gpu).\n",
    "\n",
    "Data Parallelism requires replicating model on different servers. In general there are two ways of doing this:\n",
    ">* **In-graph replication**. In this approach, the server builds a single `tf.Graph` that contains one set of parameters (in `tf.Variable` nodes pinned to `/job:ps`); and multiple copies of the compute-intensive part of the model, each pinned to a different task in `/job:worker`.  \n",
    "* **Between-graph replication**. In this approach, there is a separate server for each `/job:worker` task, typically in the same process as the `worker` task. Each server builds a similar graph containing the parameters (pinned to `/job:ps` as before using `tf.train.replica_device_setter` to map them deterministically to the same tasks); and a single copy of the compute-intensive part of the model, pinned to the local task in `/job:worker`.\n",
    "\n",
    "In a word, in **in-graph replication** all servers run the same graph; in **between-graph replication** each server runs a graph containing only the shared parameters, and whatever variables and operations are relevant to that individual server. It is obvious that the latter is the recommended. The above code replicates the model in this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Launch a distributed TensorFlow session via tf.train.MonitoredTrainingSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can start a TensorFlow session. In distributed settings, `tf.train.MonitoredTrainingSession` is used instead of `tf.Session`. `tf.train.MonitoredTrainingSession` is defined as:\n",
    "```\n",
    "tf.train.MonitoredTrainingSession(\n",
    "    master='',\n",
    "    is_chief=True,\n",
    "    checkpoint_dir=None,\n",
    "    scaffold=None,\n",
    "    hooks=None,\n",
    "    chief_only_hooks=None,\n",
    "    save_checkpoint_secs=USE_DEFAULT,\n",
    "    save_summaries_steps=USE_DEFAULT,\n",
    "    save_summaries_secs=USE_DEFAULT,\n",
    "    config=None,\n",
    "    stop_grace_period_secs=120,\n",
    "    log_step_count_steps=100,\n",
    "    max_wait_secs=7200,\n",
    "    save_checkpoint_steps=USE_DEFAULT\n",
    ")\n",
    "```\n",
    "From its args we can see that it takes care of graph initializing, checkpoint saving, TensorBoard summaries exporting, and session configuring. Beside these, `tf.train.MonitoredTrainingSession` also takes care of session starting and stopping with `hooks`. By passing a `tf.train.StopAtStepHook` to it, the last step of training is defined, after which all servers will be shut down.\n",
    "\n",
    "On arg is `is_chief`. In distributed TensorFlow's `ps-worker` architecture, one of the `workers` is appointed as `chief worker`.\n",
    "> For a chief, this utility sets proper session initializer/restorer. It also creates hooks related to checkpoint and summary saving. For workers, this utility sets proper session creator which waits for the chief to initialize/restore. \n",
    "\n",
    "The code looks as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The StopAtStepHook handles stopping after running given steps.\n",
    "hooks = [tf.train.StopAtStepHook(last_step=100000)]\n",
    "config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "with tf.train.MonitoredTrainingSession(\n",
    "    config=config,\n",
    "    master=server.target,\n",
    "    is_chief=(task_index == 0),  # appoint worker 0 as chief worker\n",
    "    checkpoint_dir='tem/train_logs',\n",
    "    hooks=hooks\n",
    ") as sess:\n",
    "    sess.run(ops)  # run operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's put everything together: distributed TensorFlow framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# setup hosts\n",
    "ps_hosts = ['10.189.253.1:2222']\n",
    "worker_hosts = ['10.189.253.2:2222', '10.189.253.3:2222']\n",
    "\n",
    "# create a cluster from the parameter server and worker hosts.\n",
    "cluster = tf.train.ClusterSpec({\n",
    "    'ps': ps_hosts,\n",
    "    'worker': worker_hosts\n",
    "})\n",
    "\n",
    "\n",
    "def worker(job_name, task_index):\n",
    "    # create server \n",
    "    server = tf.train.Server(\n",
    "        server_or_cluster_def=cluster,\n",
    "        job_name=job_name,\n",
    "        task_index=task_index\n",
    "    )\n",
    "\n",
    "    if job_name == 'ps':\n",
    "        print('Start Parameter Sever: ', task_index)\n",
    "        server.join()\n",
    "    elif job_name == 'worker':\n",
    "        # Assigns ops to the local worker by default.\n",
    "        with tf.device(\n",
    "                tf.train.replica_device_setter(\n",
    "                    worker_device='job:worker/task:{}'.format(task_index),\n",
    "                    cluster=cluster\n",
    "                )\n",
    "        ):\n",
    "            # count the number of updates\n",
    "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "            # TODO: build the graph\n",
    "            input_placeholder = ...  # placeholders\n",
    "            layer = ...  # neural network layers\n",
    "            prediction = ...  # predictions\n",
    "            loss = ...  # loss\n",
    "            train_op = ...  # train operation\n",
    "\n",
    "            # create TensorBoard summaries\n",
    "            tf.summary.scalar('loss', loss)\n",
    "\n",
    "            # merge all summaries into a single operation which we can execute in a session\n",
    "            summary_op = tf.summary.merge_all()\n",
    "\n",
    "        # the StopAtStepHook handles stopping after running given steps\n",
    "        hooks = [tf.train.StopAtStepHook(last_step=100000)]\n",
    "        \n",
    "        # session configuration\n",
    "        config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "        config.gpu_options.allow_growth = True\n",
    "        \n",
    "        # start session\n",
    "        with tf.train.MonitoredTrainingSession(\n",
    "            config=config,\n",
    "            master=server.target,\n",
    "            is_chief=(task_index == task_index),\n",
    "            checkpoint_dir='tem/train_logs',\n",
    "            hooks=hooks\n",
    "        ) as sess:\n",
    "            # tf.MonitoredTrainingSession takes care of session starting and stopping\n",
    "            while not sess.should_stop():\n",
    "                # TODO: read input data and train the model\n",
    "                sess.run(train_op)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # TODO: add argparse\n",
    "    worker('ps', 0)  # or worker('worker', task_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow will not distributed the code to each server. Therefore when execute the distributed TensorFlow programs, the same code will be sent to all servers. So your script will be sent to the 'workers' and the 'ps'. Run them one all servers. Environment variables are then used to execute a certain code block on the 'workers', another on the 'ps'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Distributed TensorFlow](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/deploy/distributed.md)  \n",
    "[How to Write Distributed TensorFlow Code — with an Example on Clusterone](https://clusterone.com/blog/2017/09/13/distributed-tensorflow-clusterone/)  \n",
    "[Distributed TensorFlow: A Gentle Introduction](https://github.com/mrahtz/distributed_tensorflow_gentle_introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
